{
  "paper1": {
    "paper_id": "paper1",
    "title": "Attention Is All You Need",
    "authors": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar"],
    "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.",
    "keywords": ["transformer", "attention mechanism", "neural networks", "sequence modeling"],
    "year": 2017,
    "venue": "NeurIPS",
    "citations": 50000,
    "relevance_score": 0.95
  },
  "paper2": {
    "paper_id": "paper2",
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "authors": ["Jacob Devlin", "Ming-Wei Chang", "Kenton Lee"],
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.",
    "keywords": ["BERT", "transformers", "language model", "pre-training"],
    "year": 2019,
    "venue": "NAACL",
    "citations": 40000,
    "relevance_score": 0.9
  },
  "paper3": {
    "paper_id": "paper3",
    "title": "Language Models are Few-Shot Learners",
    "authors": ["Tom B. Brown", "Benjamin Mann", "Nick Ryder"],
    "abstract": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples.",
    "keywords": ["GPT-3", "language models", "few-shot learning", "transfer learning"],
    "year": 2020,
    "venue": "NeurIPS",
    "citations": 10000,
    "relevance_score": 0.85
  }
} 